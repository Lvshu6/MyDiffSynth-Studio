import torch
from PIL import Image
from diffsynth.utils.data import save_video, VideoData
from diffsynth import load_state_dict
from diffsynth.pipelines.wan_video import WanVideoPipeline, ModelConfig
from modelscope import dataset_snapshot_download
from pathlib import Path
import os

# ================== é…ç½® ==================
DEVISE = "cuda:0"
input_folder = Path("data/test")          # åŸå§‹è§†é¢‘æ‰€åœ¨ç›®å½•
flow_base = input_folder             # flow çš„æ ¹ç›®å½•ï¼ˆä¸ input_folder åŒçº§ï¼‰
flow_subdir = Path("flow_line")            # flow å­ç›®å½•å
output_folder = Path("valid/r32")              # è¾“å‡ºç›®å½•

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
output_folder.mkdir(exist_ok=True)

video_extensions = {".mp4", ".avi", ".mov", ".mkv"}

# ================== åŠ è½½æ¨¡å‹ ==================
print("Loading model...")
pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device=DEVISE,
    model_configs=[
        ModelConfig(model_id="PAI/Wan2.1-Fun-V1.1-14B-InP", origin_file_pattern="diffusion_pytorch_model*.safetensors"),
        ModelConfig(model_id="PAI/Wan2.1-Fun-V1.1-14B-InP", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth"),
        ModelConfig(model_id="PAI/Wan2.1-Fun-V1.1-14B-InP", origin_file_pattern="Wan2.1_VAE.pth"),
        ModelConfig(model_id="PAI/Wan2.1-Fun-V1.1-14B-InP", origin_file_pattern="models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth"),
    ],
    vram_limit=24.0
)
pipe.vram_management_enabled = True

# åŠ è½½ flow_line_adapter å¾®è°ƒæƒé‡
# flow_state_dict = load_state_dict("models/train/Wan2.1-Fun-V1.1-14B-InP_lora_8gpu/epoch-0.safetensors")

state_dict = load_state_dict("WanFlow/Wan2.1-Fun-V1.1-14B-InP_lora_8gpu/epoch-3.safetensors")
# åˆ†ç¦»å±äº dit å’Œ flow_line_adapter çš„å‚æ•°
lora_state_dict = {}
flow_adapter_state_dict = {}
for key, value in state_dict.items():
    if key.startswith("flow_line_patch_embedding.") or key.startswith("flow_line_blocks."):
        # å±äº flow_line_adapter çš„å‚æ•°
        flow_adapter_state_dict[key] = value
    else:
        # å±äº dit çš„å‚æ•°
        lora_state_dict[key] = value

pipe.flow_line_adapter.load_state_dict(flow_adapter_state_dict)
pipe.load_lora(pipe.dit, alpha=1,state_dict=lora_state_dict)

pipe.flow_line_adapter.to(DEVISE)  # ç¡®ä¿ adapter åœ¨æ­£ç¡®è®¾å¤‡ä¸Š

print("Model loaded successfully.")

# ================== æ‰¹é‡å¤„ç† ==================
for video_path in input_folder.iterdir():
    if video_path.suffix.lower() not in video_extensions:
        continue

    print(f"\nProcessing: {video_path.name}")

    # æ„é€ å¯¹åº”çš„ flow è·¯å¾„ï¼šbase / flow / video_name
    flow_path = flow_base / flow_subdir / video_path.name
    if not flow_path.exists():
        print(f"âš ï¸ Flow file not found: {flow_path}, skipping.")
        continue

    try:
        # åŠ è½½è§†é¢‘å’Œå…‰æµæ•°æ®
        video_data = VideoData(video_path)
        flow_data = VideoData(flow_path)

        if len(video_data) == 0 or len(flow_data) == 0:
            print(f"âš ï¸ Empty video or flow, skipping: {video_path.name}")
            continue

        img_width, img_height = video_data[0].size
        num_frames = len(video_data)

        # æ¨ç†ç”Ÿæˆè§†é¢‘
        generated_video = pipe(
            prompt="move",
            negative_prompt="",
            input_image=video_data[0],
            flow_line=flow_data,
            seed=0,
            tiled=True,
            height=img_height,
            width=img_width,
            num_inference_steps=20,
            num_frames=num_frames
        )

        # ä¿å­˜ç»“æœ
        output_path = output_folder / video_path.name
        save_video(generated_video, output_path, fps=15, quality=5)
        print(f"âœ… Saved: {output_path}")

    except Exception as e:
        print(f"âŒ Error processing {video_path.name}: {e}")
        continue

print("\nğŸ‰ Batch processing completed.")